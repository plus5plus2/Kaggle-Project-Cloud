{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load in training set...\n",
      "train_x: (1000L, 4096L)\n",
      "train_y: (1000L, 10L)\n"
     ]
    }
   ],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import load_npz\n",
    "train_x_path='train_x.npz'\n",
    "train_y_path='train_y_encoded.npz'\n",
    "test_x_path='test_x.npz'\n",
    "\n",
    "print 'load in training set...'\n",
    "train_x=np.array(load_npz(train_x_path).todense())\n",
    "train_y = np.array(load_npz(train_y_path).todense())\n",
    "\n",
    "#put random seed here\n",
    "selected_sample_index=np.random.choice(int(train_x.shape[0]),1000,replace=False)\n",
    "train_x=train_x[selected_sample_index,]\n",
    "train_y=train_y[selected_sample_index,]\n",
    "\n",
    "print 'train_x:',train_x.shape\n",
    "print 'train_y:',train_y.shape\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def loss(y,nn_output):\n",
    "    return 0.5*(sum(np.array(y)-np.array(nn_output))**2)\n",
    "\n",
    "class NN(object):\n",
    "    def __init__(self, num_neuron_per_layer,lr=0.01,batch=1,epoch=20):\n",
    "        self.lr=lr\n",
    "        self.batch=batch\n",
    "        self.epoch=epoch\n",
    "        self.num_layers = len(num_neuron_per_layer)\n",
    "        self.num_neuron_per_layer = num_neuron_per_layer#dummy neuron for the bias ignored\n",
    "        self.reset_weights_bias()\n",
    "\n",
    "    def reset_weights_bias(self):\n",
    "        self.weights=[]\n",
    "        self.bias=[]\n",
    "        for i in range(0,self.num_layers-1):\n",
    "            weight=np.random.randn(self.num_neuron_per_layer[i], self.num_neuron_per_layer[i+1])\n",
    "            self.weights.append(weight)\n",
    "            self.bias.append(np.random.randn(self.num_neuron_per_layer[i+1]))\n",
    "\n",
    "    #per sample\n",
    "    def feedforward(self, nn_input):\n",
    "        layer_output_his=[]\n",
    "        layer_output_his.append(nn_input)#to facilitate sgd, add the 0-layer input\n",
    "        layer_input=np.array(nn_input)\n",
    "        #layer_input.reshape(-1,1)\n",
    "        for w,b in zip(self.weights,self.bias):\n",
    "            layer_output = sigmoid(np.dot(w.T, layer_input)+b[0])\n",
    "            layer_output_his.append(layer_output)\n",
    "            layer_input=layer_output\n",
    "        return layer_output_his #layer_output_his[-1] is nn_output\n",
    "    \n",
    "    def gradient_descent(self,true_y,layer_output_his):\n",
    "        delta_weights=[]\n",
    "        delta_bs=[]\n",
    "        layer_output=layer_output_his[-1]\n",
    "        sig=np.dot(layer_output,np.ones(len(layer_output))-layer_output)\n",
    "        delta_base=np.dot(true_y-layer_output,sig)\n",
    "        layer_output_his=layer_output_his[:-1]\n",
    "        for w,layer_output in zip(reversed(self.weights),reversed(layer_output_his)):\n",
    "            delta_weight=-np.outer(layer_output,delta_base)\n",
    "            delta_weights.append(delta_weight)\n",
    "            delta_b=-delta_base\n",
    "            delta_bs.append(delta_b)\n",
    "            sig=np.dot(layer_output,np.ones(len(layer_output))-layer_output)\n",
    "            delta_base=-np.dot(w,np.multiply(sig,delta_base))\n",
    "        return delta_weights,delta_bs\n",
    "    \n",
    "    def batch_gradient_descent(self, input_x,input_y):\n",
    "        while input_x.shape[0]>self.batch:\n",
    "            \n",
    "            current_batch_index=np.random.choice(int(input_x.shape[0]),self.batch,replace=False)\n",
    "            batch_x=input_x[current_batch_index,]\n",
    "            batch_y=input_y[current_batch_index,]\n",
    "            input_x=np.delete(input_x, current_batch_index, axis=0)\n",
    "            input_y=np.delete(input_y, current_batch_index, axis=0)\n",
    "            acc_weights=0\n",
    "            acc_bias=0\n",
    "            for x,y in zip(batch_x,batch_y):\n",
    "                layer_output_his=self.feedforward(x)\n",
    "                delta_weights,delta_bs=self.gradient_descent(y,layer_output_his)\n",
    "                acc_weights=np.add(acc_weights,delta_weights)\n",
    "                acc_bias=np.add(acc_bias,delta_bs)\n",
    "            self.update_model(acc_weights,acc_bias)\n",
    "        if input_x.shape[0]>0:\n",
    "            for x,y in zip(input_x,input_y):\n",
    "                layer_output_his=self.feedforward(x)\n",
    "                delta_weights,delta_bs=self.gradient_descent(y,layer_output_his)\n",
    "                acc_weights=np.add(acc_weights,delta_weights)\n",
    "                acc_bias=np.add(acc_bias,delta_bs)\n",
    "            self.update_model(acc_weights,acc_bias)\n",
    "    \n",
    "    def update_model(self,delta_weights,delta_bs):\n",
    "        for i in range(0,self.num_layers-1):\n",
    "            self.weights[i]=np.add(self.weights[i],np.multiply(delta_weights[-(i+1)],self.lr))\n",
    "            self.bias[i]=np.add(self.bias[i],np.multiply(delta_bs[-(i+1)],self.lr))\n",
    "    \n",
    "    def split_dataset(self,input_x,input_y,num_folder=3):\n",
    "        if len(input_x)!=len(input_y):\n",
    "            print 'len(input_x) = ',len(input_x),'; len(input_y) = ',len(input_y)\n",
    "            print 'len(input_x) must be equal to len(input_y).'\n",
    "            return None\n",
    "        return (np.linspace(0,len(input_y),num_folder+1,endpoint=True)).astype(int).tolist()[1:]\n",
    "        \n",
    "        \n",
    "    def fit_model(self,input_x,input_y,cv=False):\n",
    "        if cv:\n",
    "            split_index_end=self.split_dataset(input_x,input_y)\n",
    "            mse_his=[]\n",
    "            start=0\n",
    "            for end in split_index_end:\n",
    "                valid_x=input_x[start:end]\n",
    "                valid_y=input_y[start:end]\n",
    "                train_x=np.concatenate((input_x[0:start],input_x[end:]))\n",
    "                train_y=np.concatenate((input_y[0:start],input_y[end:]))\n",
    "                \n",
    "                self.reset_weights_bias()\n",
    "                \n",
    "                for i in range(0,self.epoch):\n",
    "                    print 'epoch ',i,' ...'\n",
    "                    self.batch_gradient_descent(input_x,input_y)\n",
    "                mse=0\n",
    "                for x,y in zip(valid_x,valid_y):\n",
    "                    mse+=loss(y,self.feedforward(x)[-1])\n",
    "                mse_his.append(float(mse)/len(valid_y))\n",
    "                start=end\n",
    "            return np.array(mse_his).mean()\n",
    "                        \n",
    "        else:\n",
    "            train_x=input_x\n",
    "            train_y=input_y\n",
    "            \n",
    "            \n",
    "            self.reset_weights_bias()\n",
    "                \n",
    "            for i in range(0,self.epoch):\n",
    "                print 'epoch ',i,' ...'\n",
    "                self.batch_gradient_descent(input_x,input_y)\n",
    "            \n",
    "            mse=0\n",
    "            for x,y in zip(train_x,train_y):\n",
    "                mse+=loss(y,self.feedforward(x)[-1])\n",
    "            return float(mse)/len(train_y)\n",
    "    \n",
    "    #per sample\n",
    "    def predict(self,x):\n",
    "        return np.argmax(self.feedforward(x)[-1])\n",
    "    \n",
    "def grid_search_model(input_x,input_y,lr_candidates=[0.01],nn_candidates=[[4096,100,10]]):\n",
    "    #an exmaple with lr\n",
    "    #init models\n",
    "    min_mse=float('inf')\n",
    "    min_lr=None\n",
    "    min_cc=None\n",
    "    c_mse=None\n",
    "    log=[]\n",
    "    for c_nn in nn_candidates:\n",
    "        for c_lr in lr_candidates:\n",
    "            \n",
    "            n=NN(num_neuron_per_layer=c_nn,lr=c_lr)\n",
    "            c_mse=n.fit_model(input_x,input_y,cv=True)\n",
    "            if min_mse>c_mse:\n",
    "                min_mse=c_mse\n",
    "                min_lr=c_lr\n",
    "                min_nn=c_nn\n",
    "            print 'best nn=',c_nn,'  c_lr=',c_lr, ' c_mse=',c_mse\n",
    "            c_log=[]\n",
    "            c_log.append(c_nn)\n",
    "            c_log.append(c_lr)\n",
    "            c_log.append(c_mse)\n",
    "            log.append(c_log)\n",
    "    return min_lr,min_nn,log\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0  ...\n",
      "epoch  1  ...\n",
      "epoch  2  ...\n",
      "epoch  3  ...\n"
     ]
    }
   ],
   "source": [
    "lr_candidate =[0.001,0.01,0.1,1]\n",
    "nn_candidate=[[4096,128,10],[4096,512,10],[4096,1024,10],[4096,2048,10],[4096,128,128,10],[4096,128,512,10],[4096,128,1024,10],[4096,512,128,10],[4096,512,512,10],[4096,512,1024,10]]\n",
    "#nn_candidate=[[4096,128,10],[4096,512,10],[4096,128,128,10],[4096,128,512,10]]\n",
    "best_lr,best_nn,log=grid_search_model(train_x,train_y,lr_candidates=lr_candidate,nn_candidates=nn_candidate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print log\n",
    "df = pd.DataFrame(log)\n",
    "df.to_csv(\"tuning_log.csv\",header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build final model configured with best hyperparameter on whole training data\n",
    "n=NN(num_neuron_per_layer=best_nn,lr=best_lr)\n",
    "f_mse=n.fit_model(train_x,train_y,cv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print training accuracy\n",
    "acc=0\n",
    "for x,y in zip(train_x,train_y):\n",
    "    predict_y=n.predict(x)\n",
    "    if predict_y==np.argmax(y):\n",
    "        acc+=1\n",
    "print float(acc)/len(train_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#load in test data\n",
    "print 'load in dummy test set...'\n",
    "test_x=train_x[100:]\n",
    "test_y=train_y[100:]\n",
    "print 'test_x:',test_x.shape\n",
    "print 'test_y:',test_y.shape\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#print testing accuracy\n",
    "acc=0\n",
    "for x,y in zip(test_x,test_y):\n",
    "    predict_y=n.predict(x)\n",
    "    if predict_y==np.argmax(y):\n",
    "        acc+=1\n",
    "print float(acc)/len(test_x)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "current_batch_index=np.random.choice(int(train_x.shape[0]),1,replace=False)\n",
    "print current_batch_index\n",
    "batch_x=train_x[current_batch_index,]\n",
    "print batch_x\n",
    "batch_y=train_y[current_batch_index,]\n",
    "print batch_y\n",
    "#input_x=np.delete(train_x, current_batch_index, axis=0)\n",
    "#input_y=np.delete(train_y, current_batch_index, axis=0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print best_model\n",
    "print n.lr\n",
    "print n.num_neuron_per_layer\n",
    "print n.batch\n",
    "print n.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=pd.read_csv('tuning_log.csv')\n",
    "r.columns =['nn','lr','loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.groupby('nn').groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r.groupby('lr').groups\n",
    "for lr in lr_candidate:\n",
    "    print r.groupby('lr').get_group(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nn in nn_candidate:\n",
    "    print r.groupby('nn').get_group(str(nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.groupby('lr').get_group(0.01).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in real test data\n",
    "print 'load in real test set...'\n",
    "test_x=np.array(load_npz(test_x_path).todense())\n",
    "result=[]\n",
    "print 'test_x:',test_x.shape\n",
    "for index, x in enumerate(test_x):\n",
    "    predict_y=n.predict(x)\n",
    "    result.append([index,predict_y])\n",
    "result_df = pd.DataFrame(result)\n",
    "result_df.columns =['Id','Label']\n",
    "result_df.to_csv(\"test_y_predict.csv\",header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
